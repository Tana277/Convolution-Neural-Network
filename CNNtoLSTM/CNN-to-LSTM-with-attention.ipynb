{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:34:19.308073Z",
          "iopub.status.busy": "2024-03-31T16:34:19.307759Z",
          "iopub.status.idle": "2024-03-31T16:34:19.320658Z",
          "shell.execute_reply": "2024-03-31T16:34:19.319771Z",
          "shell.execute_reply.started": "2024-03-31T16:34:19.308049Z"
        },
        "id": "rQZ3u_3Pamce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data_location=\"/kaggle/input/flickr8kimagescaptions/flickr8k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:34:50.238444Z",
          "iopub.status.busy": "2024-03-31T16:34:50.238028Z",
          "iopub.status.idle": "2024-03-31T16:34:51.496999Z",
          "shell.execute_reply": "2024-03-31T16:34:51.496006Z",
          "shell.execute_reply.started": "2024-03-31T16:34:50.23841Z"
        },
        "id": "NmoN3zKWamce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!ls {data_location}/images | head -4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:34:58.304616Z",
          "iopub.status.busy": "2024-03-31T16:34:58.304224Z",
          "iopub.status.idle": "2024-03-31T16:34:59.281068Z",
          "shell.execute_reply": "2024-03-31T16:34:59.280273Z",
          "shell.execute_reply.started": "2024-03-31T16:34:58.304582Z"
        },
        "id": "SEmGShPgamce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBAnplksamce"
      },
      "source": [
        "# Read and process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:35:15.215407Z",
          "iopub.status.busy": "2024-03-31T16:35:15.21441Z",
          "iopub.status.idle": "2024-03-31T16:35:15.311604Z",
          "shell.execute_reply": "2024-03-31T16:35:15.310753Z",
          "shell.execute_reply.started": "2024-03-31T16:35:15.215372Z"
        },
        "id": "RXbvclgTamcf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(data_location+\"/captions.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:35:16.38134Z",
          "iopub.status.busy": "2024-03-31T16:35:16.380937Z",
          "iopub.status.idle": "2024-03-31T16:35:16.386852Z",
          "shell.execute_reply": "2024-03-31T16:35:16.385912Z",
          "shell.execute_reply.started": "2024-03-31T16:35:16.381306Z"
        },
        "id": "SWjacXxnamcf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir(\"/kaggle/input/flickr8kimagescaptions/flickr8k\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:35:17.71678Z",
          "iopub.status.busy": "2024-03-31T16:35:17.716054Z",
          "iopub.status.idle": "2024-03-31T16:35:17.733311Z",
          "shell.execute_reply": "2024-03-31T16:35:17.73235Z",
          "shell.execute_reply.started": "2024-03-31T16:35:17.716748Z"
        },
        "id": "bcQVIcPXamcf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wppENzLramcf"
      },
      "source": [
        "# Define dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:35:19.125905Z",
          "iopub.status.busy": "2024-03-31T16:35:19.125181Z",
          "iopub.status.idle": "2024-03-31T16:35:28.681399Z",
          "shell.execute_reply": "2024-03-31T16:35:28.680462Z",
          "shell.execute_reply.started": "2024-03-31T16:35:19.125875Z"
        },
        "id": "4D7x1jgIamcf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe-TxsAqamcf"
      },
      "source": [
        "## Using spacy pretrained model for tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:35:45.344438Z",
          "iopub.status.busy": "2024-03-31T16:35:45.343513Z",
          "iopub.status.idle": "2024-03-31T16:35:46.542378Z",
          "shell.execute_reply": "2024-03-31T16:35:46.541547Z",
          "shell.execute_reply.started": "2024-03-31T16:35:45.344399Z"
        },
        "id": "H_Dq6w4namcf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:36:04.554543Z",
          "iopub.status.busy": "2024-03-31T16:36:04.553775Z",
          "iopub.status.idle": "2024-03-31T16:36:04.56425Z",
          "shell.execute_reply": "2024-03-31T16:36:04.563109Z",
          "shell.execute_reply.started": "2024-03-31T16:36:04.554508Z"
        },
        "id": "pwN1NUpDamcf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self,threshold_freq):\n",
        "        #setting the pre-reserved tokens int to string tokens\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "\n",
        "        #string to int tokens\n",
        "        #its reverse dict self.itos\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "\n",
        "        self.freq_threshold = threshold_freq\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self,text):\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:36:07.461939Z",
          "iopub.status.busy": "2024-03-31T16:36:07.461095Z",
          "iopub.status.idle": "2024-03-31T16:36:07.470679Z",
          "shell.execute_reply": "2024-03-31T16:36:07.469704Z",
          "shell.execute_reply.started": "2024-03-31T16:36:07.461905Z"
        },
        "id": "SwhQDZnZamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class flickrdataset(Dataset):\n",
        "    def __init__(self,root_dir,captions_file,transform=None,frequency_threshold=5):\n",
        "        self.root_dir=root_dir\n",
        "        self.df=pd.read_csv(captions_file)\n",
        "        self.transform=transform\n",
        "        self.imgs=self.df[\"image\"]\n",
        "        self.captions=self.df[\"caption\"]\n",
        "\n",
        "        self.vocab = Vocab(frequency_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self,idx):\n",
        "        captions=self.captions[idx]\n",
        "        images=self.imgs[idx]\n",
        "        img=Image.open(self.root_dir+\"/\"+images).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img=self.transform(img)\n",
        "        cap_vec=[]\n",
        "        cap_vec+=[self.vocab.stoi[\"<SOS>\"]]\n",
        "        cap_vec+=self.vocab.numericalize(captions)\n",
        "        cap_vec+=[self.vocab.stoi[\"<EOS>\"]]\n",
        "        return img,torch.tensor(cap_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXdZ_agHamcg"
      },
      "source": [
        "## pipeline to process image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:36:35.4386Z",
          "iopub.status.busy": "2024-03-31T16:36:35.437527Z",
          "iopub.status.idle": "2024-03-31T16:36:35.444236Z",
          "shell.execute_reply": "2024-03-31T16:36:35.44319Z",
          "shell.execute_reply.started": "2024-03-31T16:36:35.438555Z"
        },
        "id": "aeTYsJgAamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# transform pipeline to process image\n",
        "transforms=T.Compose([T.Resize(256),\n",
        "    T.RandomCrop(224),\n",
        "    T.ToTensor()\n",
        "    #T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "                     ]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:36:55.779689Z",
          "iopub.status.busy": "2024-03-31T16:36:55.778806Z",
          "iopub.status.idle": "2024-03-31T16:36:55.78525Z",
          "shell.execute_reply": "2024-03-31T16:36:55.784205Z",
          "shell.execute_reply.started": "2024-03-31T16:36:55.779657Z"
        },
        "id": "Pwg5zyszamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_image(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:37:26.39154Z",
          "iopub.status.busy": "2024-03-31T16:37:26.390644Z",
          "iopub.status.idle": "2024-03-31T16:37:28.204208Z",
          "shell.execute_reply": "2024-03-31T16:37:28.203345Z",
          "shell.execute_reply.started": "2024-03-31T16:37:26.391501Z"
        },
        "id": "CsFQkm_-amcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset =  flickrdataset(\n",
        "    root_dir = data_location+\"/images\",\n",
        "    captions_file = data_location+\"/captions.txt\",\n",
        "    transform=transforms\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:37:30.230371Z",
          "iopub.status.busy": "2024-03-31T16:37:30.229498Z",
          "iopub.status.idle": "2024-03-31T16:37:30.957629Z",
          "shell.execute_reply": "2024-03-31T16:37:30.95672Z",
          "shell.execute_reply.started": "2024-03-31T16:37:30.230336Z"
        },
        "id": "PmVBXAqtamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "img,cap=dataset[0]\n",
        "show_image(img,\"Image\")\n",
        "print(\"Token:\",cap)\n",
        "print(\"Sentence:\")\n",
        "print([dataset.vocab.itos[token] for token in cap.tolist()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:39:25.448028Z",
          "iopub.status.busy": "2024-03-31T16:39:25.447644Z",
          "iopub.status.idle": "2024-03-31T16:39:25.463101Z",
          "shell.execute_reply": "2024-03-31T16:39:25.462029Z",
          "shell.execute_reply.started": "2024-03-31T16:39:25.447999Z"
        },
        "id": "7ZbPsmUTamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "NUM_WORKER = 2\n",
        "\n",
        "#token to represent the padding\n",
        "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "def collate_function(batch):\n",
        "\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs,dim=0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=dataset.vocab.stoi[\"<PAD>\"])\n",
        "    return imgs,targets\n",
        "\n",
        "validation_split = .2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                           sampler=train_sampler,num_workers=NUM_WORKER,collate_fn=collate_function)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                                sampler=valid_sampler,num_workers=NUM_WORKER,collate_fn=collate_function)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:39:31.169929Z",
          "iopub.status.busy": "2024-03-31T16:39:31.169146Z",
          "iopub.status.idle": "2024-03-31T16:39:40.891504Z",
          "shell.execute_reply": "2024-03-31T16:39:40.890632Z",
          "shell.execute_reply.started": "2024-03-31T16:39:31.169899Z"
        },
        "id": "RbbAal3Ramcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataiter=next(iter(train_loader))\n",
        "images, captions_ = dataiter\n",
        "\n",
        "for k in range(4):\n",
        "    img,captions=images[k],captions_[k]\n",
        "    show_image(img,\" \".join([dataset.vocab.itos[token] for token in captions.tolist() if token!=dataset.vocab.stoi[\"<EOS>\"]and token!=dataset.vocab.stoi[\"<PAD>\"]]))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDYzCnOaamcg"
      },
      "source": [
        "# Define model classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:40:04.05959Z",
          "iopub.status.busy": "2024-03-31T16:40:04.059213Z",
          "iopub.status.idle": "2024-03-31T16:40:04.064612Z",
          "shell.execute_reply": "2024-03-31T16:40:04.063616Z",
          "shell.execute_reply.started": "2024-03-31T16:40:04.05956Z"
        },
        "id": "5PBRKuPyamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision.models.resnet import ResNet50_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PaHwoEsamcg"
      },
      "source": [
        "## Normal attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N8g75xbamcg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        for p in resnet.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(features.size(0), -1, features.size(-1))\n",
        "        return features  # (batch_size, 49, 2048)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.attention = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, features, hidden_state):\n",
        "        att = self.attention(hidden_state)  # (batch_size, encoder_dim)\n",
        "        att = att.unsqueeze(2)  # (batch_size, encoder_dim, 1)\n",
        "        scores = torch.bmm(features, att).squeeze(2)  # (batch_size, 49)\n",
        "        alpha = self.softmax(scores)  # (batch_size, 49)\n",
        "        context = torch.bmm(features.transpose(1, 2), alpha.unsqueeze(2)).squeeze(2)  # (batch_size, encoder_dim)\n",
        "        return alpha, context\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(encoder_dim, decoder_dim)\n",
        "\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.lstm_cell = nn.LSTMCell(embed_size + encoder_dim, decoder_dim, bias=True)\n",
        "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeds = self.embedding(captions)\n",
        "        h, c = self.init_hidden_state(features)\n",
        "\n",
        "        seq_length = len(captions[0]) - 1\n",
        "        batch_size = captions.size(0)\n",
        "\n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(features.device)\n",
        "        alphas = torch.zeros(batch_size, seq_length, features.size(1)).to(features.device)\n",
        "\n",
        "        for s in range(seq_length):\n",
        "            alpha, context = self.attention(features, h)\n",
        "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            preds[:, s] = output\n",
        "            alphas[:, s] = alpha\n",
        "\n",
        "        return preds, alphas\n",
        "\n",
        "    def generate_caption(self, features, max_len=20, vocab=None):\n",
        "        batch_size = features.size(0)\n",
        "        h, c = self.init_hidden_state(features)\n",
        "        alphas = []\n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1, -1).to(features.device)\n",
        "        embeds = self.embedding(word)\n",
        "        captions = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            alpha, context = self.attention(features, h)\n",
        "            alphas.append(alpha.cpu().detach().numpy())\n",
        "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            output = output.view(batch_size, -1)\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        return [vocab.itos[idx] for idx in captions], alphas\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SbrpXZkamcg"
      },
      "source": [
        "## Bahadnau attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:40:24.32814Z",
          "iopub.status.busy": "2024-03-31T16:40:24.327706Z",
          "iopub.status.idle": "2024-03-31T16:40:24.336Z",
          "shell.execute_reply": "2024-03-31T16:40:24.334838Z",
          "shell.execute_reply.started": "2024-03-31T16:40:24.328109Z"
        },
        "id": "M6NV5OZOamcg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN,self).__init__()\n",
        "        resnet=models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        for p in resnet.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        modules=list(resnet.children())[:-2]\n",
        "        self.resnet=nn.Sequential(*modules)\n",
        "    def forward(self,images):\n",
        "        features=self.resnet(images)\n",
        "        features=features.permute(0,2,3,1)\n",
        "        features=features.view(features.size(0),-1,features.size(-1))\n",
        "        return features #(batch_size,49,2048)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:40:27.631475Z",
          "iopub.status.busy": "2024-03-31T16:40:27.631089Z",
          "iopub.status.idle": "2024-03-31T16:40:27.639792Z",
          "shell.execute_reply": "2024-03-31T16:40:27.638677Z",
          "shell.execute_reply.started": "2024-03-31T16:40:27.631445Z"
        },
        "id": "6rC-5-znamch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class BahadnauAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BahadnauAttention,self).__init__()\n",
        "        self.U=nn.Linear(512,512)\n",
        "        self.W=nn.Linear(2048,512)\n",
        "        self.v=nn.Linear(512,1)\n",
        "        self.tanh=nn.Tanh()\n",
        "        self.softmax=nn.Softmax(1)\n",
        "\n",
        "    def forward(self,features,hidden_states):\n",
        "        U_hidden=self.U(hidden_states)#(batch_size,512)\n",
        "        W_features=self.W(features)#(batch_size,49,512)\n",
        "        attention=self.tanh(U_hidden.unsqueeze(1)+W_features)\n",
        "        e=self.v(attention).squeeze(2) #(batch_size,1)\n",
        "        alpha=self.softmax(e)\n",
        "        context=(features*alpha.unsqueeze(2)).sum(1)\n",
        "        return alpha,context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:45:56.59675Z",
          "iopub.status.busy": "2024-03-31T16:45:56.596361Z",
          "iopub.status.idle": "2024-03-31T16:45:56.616777Z",
          "shell.execute_reply": "2024-03-31T16:45:56.615875Z",
          "shell.execute_reply.started": "2024-03-31T16:45:56.59672Z"
        },
        "id": "qOu_Apw7amch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DecoderRNNBahadnau(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        #save the model param\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
        "        self.attention = BahadnauAttention()\n",
        "\n",
        "\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "\n",
        "\n",
        "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        #vectorize the caption\n",
        "        embeds = self.embedding(captions)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
        "\n",
        "        #get the seq length to iterate\n",
        "        seq_length = len(captions[0])-1 #Exclude the last one\n",
        "        batch_size = captions.size(0)\n",
        "        num_features = features.size(1)\n",
        "\n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
        "\n",
        "        for s in range(seq_length):\n",
        "            alpha,context = self.attention(features, h)\n",
        "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "\n",
        "            output = self.fcn(self.drop(h))\n",
        "\n",
        "            preds[:,s] = output\n",
        "            alphas[:,s] = alpha\n",
        "\n",
        "\n",
        "        return preds, alphas\n",
        "\n",
        "    def generate_caption(self,features,max_len=20,vocab=None):\n",
        "        # Inference part\n",
        "        # Given the image features generate the captions\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
        "\n",
        "        alphas = []\n",
        "\n",
        "        #starting input\n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "\n",
        "        captions = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            alpha,context = self.attention(features, h)\n",
        "\n",
        "\n",
        "            #store the apla score\n",
        "            alphas.append(alpha.cpu().detach().numpy())\n",
        "\n",
        "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            output = output.view(batch_size,-1)\n",
        "\n",
        "\n",
        "            #select the word with most val\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "\n",
        "            #save the generated word\n",
        "            captions.append(predicted_word_idx.item())\n",
        "\n",
        "            #end if <EOS detected>\n",
        "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            #send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "\n",
        "        #covert the vocab idx to words and return sentence\n",
        "        return [vocab.itos[idx] for idx in captions],alphas\n",
        "\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:45:57.125544Z",
          "iopub.status.busy": "2024-03-31T16:45:57.124609Z",
          "iopub.status.idle": "2024-03-31T16:45:57.132167Z",
          "shell.execute_reply": "2024-03-31T16:45:57.131097Z",
          "shell.execute_reply.started": "2024-03-31T16:45:57.125497Z"
        },
        "id": "xmVIV899amch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderBahadnau(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        self.decoder = DecoderRNNBahadnau(\n",
        "            embed_size=embed_size,\n",
        "            vocab_size = len(dataset.vocab),\n",
        "            attention_dim=attention_dim,\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kxpoFHGamch"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderAttn(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        self.decoder = DecoderRNN(\n",
        "            embed_size=embed_size,\n",
        "            vocab_size = len(dataset.vocab),\n",
        "            attention_dim=attention_dim,\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:53:38.074651Z",
          "iopub.status.busy": "2024-03-31T16:53:38.073964Z",
          "iopub.status.idle": "2024-03-31T16:53:38.081275Z",
          "shell.execute_reply": "2024-03-31T16:53:38.08012Z",
          "shell.execute_reply.started": "2024-03-31T16:53:38.074608Z"
        },
        "id": "GoqAw1F0amch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "embed_size=300\n",
        "vocab_size = len(dataset.vocab)\n",
        "\n",
        "# attention_dim=64\n",
        "# encoder_dim=512\n",
        "# decoder_dim=128\n",
        "\n",
        "attention_dim=256\n",
        "encoder_dim=2048\n",
        "decoder_dim=512\n",
        "\n",
        "learning_rate = 3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T16:53:38.285927Z",
          "iopub.status.busy": "2024-03-31T16:53:38.285025Z",
          "iopub.status.idle": "2024-03-31T16:53:38.290905Z",
          "shell.execute_reply": "2024-03-31T16:53:38.289858Z",
          "shell.execute_reply.started": "2024-03-31T16:53:38.285892Z"
        },
        "id": "ZdST3AfRamch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:12:29.28076Z",
          "iopub.status.busy": "2024-03-31T17:12:29.280409Z",
          "iopub.status.idle": "2024-03-31T17:12:29.996984Z",
          "shell.execute_reply": "2024-03-31T17:12:29.996078Z",
          "shell.execute_reply.started": "2024-03-31T17:12:29.280736Z"
        },
        "id": "oEnEMhALamch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "modelAttn = EncoderDecoderAttn(\n",
        "    embed_size=embed_size,\n",
        "    vocab_size = vocab_size,\n",
        "    attention_dim=attention_dim,\n",
        "    encoder_dim=encoder_dim,\n",
        "    decoder_dim=decoder_dim\n",
        ").to(device)\n",
        "\n",
        "criterionAttn = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizerAttn = optim.Adam(modelAttn.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:12:29.99896Z",
          "iopub.status.busy": "2024-03-31T17:12:29.998622Z",
          "iopub.status.idle": "2024-03-31T17:12:30.70814Z",
          "shell.execute_reply": "2024-03-31T17:12:30.707094Z",
          "shell.execute_reply.started": "2024-03-31T17:12:29.998933Z"
        },
        "id": "TsANvdq-amch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "modelBahadnau = EncoderDecoderBahadnau(\n",
        "    embed_size=embed_size,\n",
        "    vocab_size = vocab_size,\n",
        "    attention_dim=attention_dim,\n",
        "    encoder_dim=encoder_dim,\n",
        "    decoder_dim=decoder_dim\n",
        ").to(device)\n",
        "\n",
        "criterionBahadnau = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizerBahadnau = optim.Adam(modelBahadnau.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:08:03.524313Z",
          "iopub.status.busy": "2024-03-31T17:08:03.523358Z",
          "iopub.status.idle": "2024-03-31T17:08:03.530403Z",
          "shell.execute_reply": "2024-03-31T17:08:03.529067Z",
          "shell.execute_reply.started": "2024-03-31T17:08:03.52427Z"
        },
        "id": "MSSkK03-amch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_model(model, name,num_epochs):\n",
        "    model_state = {\n",
        "        'num_epochs':num_epochs,\n",
        "        'embed_size':embed_size,\n",
        "        'vocab_size':len(dataset.vocab),\n",
        "        'attention_dim':attention_dim,\n",
        "        'encoder_dim':encoder_dim,\n",
        "        'decoder_dim':decoder_dim,\n",
        "        'state_dict':model.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(model_state,f'{name}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:17:42.157891Z",
          "iopub.status.busy": "2024-03-31T17:17:42.157522Z",
          "iopub.status.idle": "2024-03-31T17:17:42.169196Z",
          "shell.execute_reply": "2024-03-31T17:17:42.168008Z",
          "shell.execute_reply.started": "2024-03-31T17:17:42.15786Z"
        },
        "id": "yNyQVbUQamch",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model,train_loader,validation_loader,optimizer,criterion,num_epochs, print_every, name = 'normal'):\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        losses_per_epoch = 0\n",
        "\n",
        "        for idx, (image, captions) in enumerate(train_loader):\n",
        "            image, captions = image.to(device), captions.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = model.encoder(image)\n",
        "            outputs, attentions = model.decoder(features, captions)\n",
        "\n",
        "            # Calculate loss\n",
        "            targets = captions[:, 1:]\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "            losses_per_epoch += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (idx + 1) % print_every == 0:\n",
        "                print(\"Epoch: {} loss: {:.5f}\".format(epoch, loss.item()))\n",
        "\n",
        "                # Generate caption\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    dataiter = iter(validation_loader)\n",
        "                    img, _ = next(dataiter)\n",
        "                    features = model.encoder(img[0:1].to(device))\n",
        "                    caps, _ = model.decoder.generate_caption(features, vocab=dataset.vocab)\n",
        "                    caption = ' '.join(caps)\n",
        "                    show_image(img[0], title=caption)\n",
        "\n",
        "                model.train()\n",
        "#             break\n",
        "        losses.append(losses_per_epoch)\n",
        "    save_model(model,name,epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Gt_-hlamch"
      },
      "source": [
        "### Train model with normal attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:17:42.599849Z",
          "iopub.status.busy": "2024-03-31T17:17:42.599011Z",
          "iopub.status.idle": "2024-03-31T17:17:51.838003Z",
          "shell.execute_reply": "2024-03-31T17:17:51.836808Z",
          "shell.execute_reply.started": "2024-03-31T17:17:42.599809Z"
        },
        "id": "t_vtfWExamci",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "print_every = 100\n",
        "\n",
        "train(modelAttn,\n",
        "      train_loader,\n",
        "      validation_loader,\n",
        "      optimizerAttn,\n",
        "      criterionAttn,\n",
        "      num_epochs,\n",
        "      print_every,\n",
        "      name = 'Normal Attention')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sguR8f9Famci"
      },
      "source": [
        "### Train model with Bahadnau attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:17:51.84056Z",
          "iopub.status.busy": "2024-03-31T17:17:51.84025Z",
          "iopub.status.idle": "2024-03-31T17:18:00.817307Z",
          "shell.execute_reply": "2024-03-31T17:18:00.816125Z",
          "shell.execute_reply.started": "2024-03-31T17:17:51.840533Z"
        },
        "id": "kl1gz3fxamci",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train(modelBahadnau,\n",
        "      train_loader,\n",
        "      validation_loader,\n",
        "      optimizerBahadnau,\n",
        "      criterionBahadnau,\n",
        "      num_epochs,\n",
        "      print_every,\n",
        "      name = 'Bahadnau Attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn7DanFAamci"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:25:20.264585Z",
          "iopub.status.busy": "2024-03-31T17:25:20.264206Z",
          "iopub.status.idle": "2024-03-31T17:25:25.013101Z",
          "shell.execute_reply": "2024-03-31T17:25:25.011924Z",
          "shell.execute_reply.started": "2024-03-31T17:25:20.264552Z"
        },
        "id": "UpKP8hhAamci",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#testing\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "modelAttn.eval()\n",
        "img,caption=next(iter(validation_loader))\n",
        "img_=img[50]\n",
        "print(img.size())\n",
        "show_image(img_,\" \".join([dataset.vocab.itos[k] for k in caption[50].tolist() if dataset.vocab.itos[k]!=\"<PAD>\"]))\n",
        "features = modelAttn.encoder(img_.unsqueeze(0).to(device))\n",
        "caps,alphas = modelAttn.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "caption_ = ' '.join(caps)\n",
        "reference=[[dataset.vocab.itos[k] for k in caption[50].tolist() if dataset.vocab.itos[k]!=\"<PAD>\"]]\n",
        "candidate=caps\n",
        "print(reference,candidate)\n",
        "score=sentence_bleu(reference,candidate,weights=(1,0,0,0))\n",
        "print(\"Generated caption : \",caption_)\n",
        "print(\"Bleu 1 gram score : \",score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:27:14.631304Z",
          "iopub.status.busy": "2024-03-31T17:27:14.630611Z",
          "iopub.status.idle": "2024-03-31T17:27:14.6419Z",
          "shell.execute_reply": "2024-03-31T17:27:14.640932Z",
          "shell.execute_reply.started": "2024-03-31T17:27:14.631267Z"
        },
        "id": "c64KvkIKamci",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#checking corpus bleu score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def score_model(model,validation_loader):\n",
        "    references=[]\n",
        "    candidates=[]\n",
        "    for idx,(image,caption) in enumerate(iter(validation_loader)):\n",
        "        for k in range(image.size()[0]):\n",
        "            features=model.encoder(image[k].unsqueeze(0).to(device))\n",
        "            caps,alpha=model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "            reference=[dataset.vocab.itos[i] for i in caption[k].tolist() if dataset.vocab.itos[i]!=\"<PAD>\"]\n",
        "            references.append(reference)\n",
        "            candidates.append(caps)\n",
        "\n",
        "    print(\"bleu 1 score : \",corpus_bleu(references,candidates,weights=(1,0,0,0)))\n",
        "    print(\"bleu 2 score : \",corpus_bleu(references,candidates,weights=(0.5,0.5,0,0)))\n",
        "    print(\"bleu 3 score : \",corpus_bleu(references,candidates,weights=(0.33,0.33,0.33,0)))\n",
        "    print(\"bleu 4 score : \",corpus_bleu(references,candidates,weights=(0.25,0.25,0.25,0.25)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:27:15.541663Z",
          "iopub.status.busy": "2024-03-31T17:27:15.540665Z",
          "iopub.status.idle": "2024-03-31T17:27:22.872742Z",
          "shell.execute_reply": "2024-03-31T17:27:22.871566Z",
          "shell.execute_reply.started": "2024-03-31T17:27:15.541612Z"
        },
        "id": "x2c-GTq9amci",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "score_model(modelAttn, validation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-31T17:27:22.876034Z",
          "iopub.status.busy": "2024-03-31T17:27:22.875726Z",
          "iopub.status.idle": "2024-03-31T17:27:32.826123Z",
          "shell.execute_reply": "2024-03-31T17:27:32.824956Z",
          "shell.execute_reply.started": "2024-03-31T17:27:22.876005Z"
        },
        "id": "bwxCKlfKamci",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "score_model(modelBahadnau, validation_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebookf6ff6328d1",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 771078,
          "sourceId": 1328792,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30673,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
